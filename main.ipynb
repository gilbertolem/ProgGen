{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # http://pytorch.org/\n",
    "# from os.path import exists\n",
    "# from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "# platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "# cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "# accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "# !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/gilbertolem/ProgGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import chdir, getcwd\n",
    "# chdir(\"ProgGen\")\n",
    "# from sys import path\n",
    "# path.append(getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import utils.data_tools as data_tools\n",
    "from utils.nets import ProgGen_RNN\n",
    "from pickle import load, dump\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xml_directory = \"XML_Tunes/\"\n",
    "torch.manual_seed(999)\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATING TENSORS FROM MUSICXML FILES...\n",
      "\t297 tunes succesfully loaded for training.\n",
      "\t75 tunes succesfully loaded for validation.\n"
     ]
    }
   ],
   "source": [
    "# Load vocabulary\n",
    "words_text2num = load(open(\"maps/words_text2num.txt\",'rb'))\n",
    "vocab_size = len(words_text2num)\n",
    "\n",
    "# Create training data\n",
    "filters = {'author':'Charlie Parker', 'style':None}\n",
    "X_train, X_val = data_tools.musicxml2tensor(xml_directory, words_text2num, filters = filters) # (Seq x Batch x vocab_size)\n",
    "train_data = data_tools.TuneData(X_train)\n",
    "val_data = data_tools.TuneData(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Neural Net\n",
    "embed_size = 100\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "dropout = 0\n",
    "bidirectional = False\n",
    "rnn_type = 'lstm'\n",
    "model = ProgGen_RNN(rnn_type, vocab_size, embed_size, hidden_size, num_layers, dropout, bidirectional)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define loader\n",
    "sampler = torch.utils.data.RandomSampler(train_data)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = 372, sampler = sampler, num_workers = 1 if use_gpu else 4)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size = 372, num_workers = 1 if use_gpu else 4)\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    loss_fn = loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "lr = 1e-2\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------\n",
      "TRAINING MODEL... \n",
      "\n",
      "   Epoch | Training Loss | Val. Loss\n",
      "       0 |          5.55 |      3.25\n",
      "       1 |          3.53 |      3.17\n",
      "       2 |          3.51 |      2.89\n",
      "       3 |           3.1 |       2.9\n",
      "       4 |          3.01 |      2.84\n",
      "       5 |          2.95 |      2.69\n",
      "       6 |          2.83 |      2.53\n",
      "       7 |          2.71 |      2.42\n",
      "       8 |          2.61 |      2.35\n",
      "       9 |          2.52 |      2.28\n",
      "      10 |          2.44 |      2.23\n",
      "      11 |          2.36 |      2.18\n",
      "      12 |          2.29 |      2.12\n",
      "      13 |          2.21 |      2.06\n",
      "      14 |          2.13 |       2.0\n",
      "      15 |          2.05 |      1.95\n",
      "      16 |          1.99 |       1.9\n",
      "      17 |          1.93 |      1.86\n",
      "      18 |          1.88 |      1.82\n",
      "      19 |          1.84 |      1.79\n",
      "      20 |           1.8 |      1.76\n",
      "      21 |          1.77 |      1.74\n",
      "      22 |          1.74 |      1.71\n",
      "      23 |          1.71 |      1.68\n",
      "      24 |          1.68 |      1.66\n",
      "      25 |          1.65 |      1.63\n",
      "      26 |          1.62 |       1.6\n",
      "      27 |           1.6 |      1.58\n",
      "      28 |          1.58 |      1.56\n",
      "      29 |          1.56 |      1.54\n",
      "      30 |          1.54 |      1.52\n",
      "      31 |          1.52 |       1.5\n",
      "      32 |          1.51 |      1.49\n",
      "      33 |          1.49 |      1.47\n",
      "      34 |          1.48 |      1.47\n",
      "      35 |          1.47 |      1.46\n",
      "      36 |          1.46 |      1.45\n",
      "      37 |          1.45 |      1.44\n",
      "      38 |          1.44 |      1.44\n",
      "      39 |          1.42 |      1.43\n",
      "      40 |          1.41 |      1.43\n",
      "      41 |          1.41 |      1.43\n",
      "      42 |          1.39 |      1.42\n",
      "      43 |          1.39 |      1.42\n",
      "      44 |          1.38 |      1.41\n",
      "      45 |          1.37 |      1.42\n",
      "      46 |          1.37 |      1.42\n",
      "      47 |          1.36 |      1.42\n",
      "      48 |          1.36 |      1.42\n",
      "      49 |          1.35 |      1.43\n",
      "      50 |          1.34 |      1.43\n",
      "      51 |          1.33 |      1.43\n",
      "      52 |          1.33 |      1.43\n",
      "      53 |          1.32 |      1.42\n",
      "      54 |          1.32 |      1.44\n",
      "      55 |          1.31 |      1.43\n",
      "      56 |          1.31 |      1.43\n",
      "      57 |           1.3 |      1.43\n",
      "      58 |          1.29 |      1.44\n",
      "      59 |          1.29 |      1.43\n",
      "      60 |          1.28 |      1.43\n",
      "      61 |          1.28 |      1.43\n",
      "      62 |          1.28 |       1.5\n",
      "      63 |          1.29 |      1.44\n",
      "      64 |          1.29 |      1.44\n",
      "      65 |          1.28 |      1.44\n",
      "      66 |          1.28 |      1.44\n",
      "      67 |          1.27 |      1.44\n",
      "      68 |          1.26 |      1.48\n",
      "      69 |          1.26 |      1.45\n",
      "      70 |          1.26 |      1.44\n",
      "      71 |          1.26 |      1.45\n",
      "      72 |          1.26 |      1.45\n",
      "      73 |          1.25 |      1.48\n",
      "      74 |          1.24 |      1.46\n",
      "      75 |          1.24 |      1.44\n",
      "      76 |          1.24 |      1.45\n",
      "      77 |          1.22 |      1.46\n",
      "      78 |          1.22 |      1.47\n",
      "      79 |          1.22 |      1.47\n",
      "      80 |          1.22 |      1.48\n",
      "      81 |          1.21 |      1.51\n",
      "      82 |          1.22 |      1.46\n",
      "      83 |          1.22 |      1.47\n",
      "      84 |           1.2 |       1.5\n",
      "      85 |           1.2 |       1.5\n",
      "      86 |           1.2 |      1.46\n",
      "      87 |          1.19 |      1.48\n",
      "      88 |           1.2 |      1.51\n",
      "      89 |          1.19 |      1.51\n",
      "      90 |          1.18 |      1.52\n",
      "      91 |          1.17 |      1.56\n",
      "      92 |          1.17 |      1.53\n",
      "      93 |          1.18 |      1.53\n",
      "      94 |          1.17 |      1.55\n",
      "      95 |          1.17 |      1.64\n",
      "      96 |          1.16 |      1.66\n",
      "      97 |          1.16 |      1.56\n",
      "      98 |          1.17 |      1.47\n",
      "      99 |          1.21 |      1.64\n",
      "     100 |          1.15 |      1.64\n",
      "     101 |          1.16 |      1.46\n",
      "     102 |           1.2 |      1.47\n",
      "     103 |          1.19 |      1.49\n",
      "     104 |          1.18 |       1.5\n",
      "     105 |          1.16 |      1.54\n",
      "     106 |          1.17 |      1.55\n",
      "     107 |          1.18 |      1.51\n",
      "     108 |          1.17 |       1.5\n",
      "     109 |          1.17 |      1.49\n",
      "     110 |          1.17 |      1.46\n",
      "     111 |          1.15 |      1.47\n",
      "     112 |          1.15 |      1.47\n",
      "     113 |          1.14 |      1.46\n",
      "     114 |          1.14 |      1.48\n",
      "     115 |          1.15 |      1.47\n",
      "     116 |          1.14 |      1.45\n",
      "     117 |          1.14 |      1.48\n",
      "     118 |          1.13 |       1.5\n",
      "     119 |          1.12 |      1.53\n",
      "     120 |          1.12 |       1.5\n",
      "     121 |          1.11 |      1.54\n",
      "     122 |          1.11 |      1.55\n",
      "     123 |          1.11 |      1.52\n",
      "     124 |          1.11 |       1.5\n",
      "     125 |          1.13 |      1.52\n",
      "     126 |          1.11 |      1.64\n",
      "     127 |          1.12 |      1.59\n",
      "     128 |           1.1 |      1.55\n",
      "     129 |           1.1 |      1.59\n",
      "     130 |          1.11 |      1.53\n",
      "     131 |           1.1 |      1.52\n",
      "     132 |           1.1 |      1.56\n",
      "     133 |           1.1 |      1.64\n",
      "     134 |          1.09 |      1.69\n",
      "     135 |          1.09 |      1.68\n",
      "     136 |          1.08 |      1.55\n",
      "     137 |          1.08 |      1.54\n",
      "     138 |          1.07 |      1.57\n",
      "     139 |          1.08 |      1.66\n",
      "     140 |          1.07 |      1.55\n",
      "     141 |          1.07 |      1.56\n",
      "     142 |          1.07 |      1.53\n",
      "     143 |          1.06 |      1.55\n",
      "     144 |          1.06 |      1.61\n",
      "     145 |          1.06 |      1.55\n",
      "     146 |          1.06 |      1.53\n",
      "     147 |          1.05 |      1.56\n",
      "     148 |          1.04 |      1.59\n",
      "     149 |          1.05 |      1.58\n",
      "     150 |          1.06 |      1.56\n",
      "     151 |          1.05 |      1.57\n",
      "     152 |          1.04 |      1.75\n",
      "     153 |          1.04 |      1.75\n",
      "     154 |          1.05 |      1.69\n",
      "     155 |          1.04 |      1.56\n",
      "     156 |          1.04 |      1.58\n",
      "     157 |          1.05 |      1.57\n",
      "     158 |          1.04 |      1.65\n",
      "     159 |          1.03 |      1.79\n",
      "     160 |          1.06 |      1.61\n",
      "     161 |          1.03 |      1.59\n",
      "     162 |          1.02 |      1.58\n",
      "     163 |          1.04 |      1.59\n",
      "     164 |          1.04 |      1.61\n",
      "     165 |          1.02 |      1.65\n",
      "     166 |          1.01 |      1.62\n",
      "     167 |          1.01 |      1.59\n",
      "     168 |          1.02 |       1.7\n",
      "     169 |          1.01 |      1.74\n",
      "     170 |          1.02 |       1.6\n",
      "     171 |           1.0 |      1.61\n",
      "     172 |          1.02 |      1.61\n",
      "     173 |          1.02 |      1.61\n",
      "     174 |          1.02 |      1.61\n",
      "     175 |           1.0 |      1.79\n",
      "     176 |           1.0 |      1.86\n",
      "     177 |          1.01 |      1.66\n",
      "     178 |          0.99 |      1.57\n",
      "     179 |           1.0 |      1.55\n",
      "     180 |          1.02 |      1.52\n",
      "     181 |          1.02 |      1.53\n",
      "     182 |           1.0 |      1.61\n",
      "     183 |           1.0 |      1.62\n",
      "     184 |           1.0 |       1.6\n",
      "     185 |          0.99 |      1.62\n",
      "     186 |          0.99 |       1.6\n",
      "     187 |          0.98 |      1.58\n",
      "     188 |          0.99 |       1.6\n",
      "     189 |          0.99 |      1.68\n",
      "     190 |          0.99 |      1.69\n",
      "     191 |          0.97 |      1.61\n",
      "     192 |          0.97 |      1.59\n",
      "     193 |          0.97 |       1.6\n",
      "     194 |          0.97 |      1.67\n",
      "     195 |          0.96 |      1.61\n",
      "     196 |          0.96 |       1.6\n",
      "     197 |          0.96 |      1.59\n",
      "     198 |          0.96 |       1.6\n",
      "     199 |          0.98 |      1.62\n",
      "     200 |          0.96 |      1.67\n",
      "     201 |          0.97 |      1.74\n",
      "     202 |          0.97 |      1.71\n",
      "     203 |          0.97 |       1.6\n",
      "     204 |          0.96 |       1.6\n",
      "     205 |          1.01 |      1.61\n",
      "     206 |          0.97 |      1.65\n",
      "     207 |          0.96 |      1.66\n",
      "     208 |          0.95 |      1.73\n",
      "     209 |          0.95 |       1.9\n",
      "     210 |          0.94 |      1.96\n",
      "     211 |          0.96 |      1.67\n",
      "     212 |          0.94 |      1.63\n",
      "     213 |          0.94 |      1.65\n",
      "     214 |          0.94 |      1.65\n",
      "     215 |          0.94 |      1.63\n",
      "     216 |          0.94 |      1.63\n",
      "     217 |          0.94 |      1.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     218 |          0.93 |      1.64\n",
      "     219 |          0.93 |      1.63\n",
      "     220 |          0.95 |      1.69\n",
      "     221 |          0.92 |      1.71\n",
      "     222 |          0.93 |      1.69\n",
      "     223 |          0.97 |      1.67\n",
      "     224 |          0.93 |      1.68\n",
      "     225 |          0.94 |      1.99\n",
      "     226 |          0.94 |      1.68\n",
      "     227 |          0.93 |      1.66\n",
      "     228 |          0.92 |      1.66\n",
      "     229 |          0.92 |      1.67\n",
      "     230 |          0.94 |      1.68\n",
      "     231 |          0.92 |      1.71\n",
      "     232 |          0.93 |      1.75\n",
      "     233 |          0.93 |      1.74\n",
      "     234 |          0.92 |       1.7\n",
      "     235 |          0.94 |      1.63\n",
      "     236 |          0.92 |      1.63\n"
     ]
    }
   ],
   "source": [
    "from utils.training import train\n",
    "epochs = 500\n",
    "losses = train(epochs, model, optim, train_loader, val_loader, loss_fn, use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(losses[0], label='Train')\n",
    "plt.semilogy(losses[1], label='Val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "import numpy as np\n",
    "print(losses[0][-1])\n",
    "print(losses[1][-1])\n",
    "idx = np.argmin(losses[1])\n",
    "print(losses[0][idx])\n",
    "print(losses[1][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generating import generate_progression\n",
    "\n",
    "model_name = \"model\"\n",
    "initial_chord = \"4C_maj\"\n",
    "tune_len = 32\n",
    "top = 10\n",
    "\n",
    "prog = generate_progression(initial_chord, tune_len, top, model_name, verbose = False)\n",
    "print(\"Generated Progression:\\n\")\n",
    "print(prog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
